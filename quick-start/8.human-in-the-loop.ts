import { ChatOpenAI } from '@langchain/openai'
import './lib/loadEnv'
import { Annotation, Command, MemorySaver, StateGraph, interrupt } from '@langchain/langgraph'
import { ToolNode } from '@langchain/langgraph/prebuilt'
import { AIMessage, BaseMessage, HumanMessage } from '@langchain/core/messages'
import { tool } from '@langchain/core/tools'
import { z } from "zod";
import { randomUUID } from 'crypto'
import { hu } from 'zod/locales'


// ä¸€ã€Checkpointer åˆ›å»º

const devCheckpointer = new MemorySaver()

// äºŒã€LLMå®ä¾‹åŒ–

const llm = new ChatOpenAI({
    model: 'qwen3-max'
})

// ä¸‰ã€å·¥å…·çš„å®šä¹‰

const wetherSchema = z.object({ city: z.string()})

const getWether = tool(
    async (input: any, config) => {
        const { city} = input;
        return `${city} ä»Šæ—¥æ™´ï¼Œ25Â°C`;
    },
    {
        name: 'getWether',
        description: "æŸ¥è¯¢æŒ‡å®šåŸå¸‚å½“å‰å¤©æ°”ä¿¡æ¯çš„å·¥å…·ã€‚è¯·æä¾›åŸå¸‚åç§°ã€‚  å¦‚ï¼š ä¸Šæµ·ã€åŒ—äº¬",
        schema: wetherSchema
    }
)

const tools = [getWether]

// å››ã€çŠ¶æ€å®šä¹‰
const StateAnnotation = Annotation.Root({
    message: Annotation<BaseMessage[]>({
        reducer: (oldState, newState) => {
            return [...oldState, ...newState]
        },
        default() {
            return []
        },
    })
})

// äº”ã€èŠ‚ç‚¹å‡½æ•°å®šä¹‰
const llmNode = async (state: typeof StateAnnotation.State) => {
    const llmWithTools = llm.bindTools(tools)
    const response = await llmWithTools.invoke(state.message)
    return {
        messages: [response]
    }
}

// ä½¿ç”¨å†…ç½®çš„ ToolNode
const toolNode = new ToolNode(tools)

/**
 * æ¡ä»¶å‡½æ•°
*/
const shouldContinue = (state: typeof StateAnnotation.State) => {
    const lastMessage = state.message[state.message.length - 1] as AIMessage

    const value = interrupt(
        {
            customValue: 'æ˜¯å¦è°ƒç”¨å¤§æ¨¡å‹-shouldContinue'
        }
    )
    console.log("%c Line:81 ğŸŠ shouldContinue", "color:#6ec1c2", value);

    return lastMessage.tool_calls && lastMessage.tool_calls.length > 0 ? 'tools' : 'end'

}

// å…­ã€æ„å»ºå¹¶ç¼–è¯‘å›¾
export const humanInTheLoopGraph = new StateGraph(StateAnnotation)
    .addNode('llmNode', llmNode)
    .addNode('tools', toolNode)
    .addEdge('__start__', 'llmNode')
    .addConditionalEdges('llmNode', shouldContinue, {
        tools: 'tools',
        end: '__end__'
    })
    .addEdge('tools', 'llmNode')
    .compile({ checkpointer: devCheckpointer })

// ä¸ƒã€è¿è¡Œç¤ºä¾‹
async function runDemo() {
    try {
        const threadId = randomUUID()

        console.log("ğŸ“ ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šå‘é€é—®é¢˜ï¼Œç­‰å¾…ä¸­æ–­...");
        const res = await humanInTheLoopGraph.invoke({
            message: [
                new HumanMessage('ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·')
            ]
        }, {
            configurable: {
                thread_id: threadId,
            }
        })


        console.log("%c Line:63 ğŸ¥ª interrupt", "color:#f5ce50", (res as any).__interrupt__?.[0]?.value?.customValue);
        
        console.log("\nğŸ“ ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šæ¢å¤æ‰§è¡Œ...");

        const msgs = await humanInTheLoopGraph.invoke(new Command({ resume: true }), {
            configurable: {
                thread_id: threadId
            }
        })
        console.log("%c Line:101 ğŸŒ­ msgs", "color:#7f2b82", msgs);
    
    } catch (error) {
        console.error("é”™è¯¯:", error)    
    }
}

// è¿è¡Œç¤ºä¾‹
if (require.main === module) {
    runDemo()
}


